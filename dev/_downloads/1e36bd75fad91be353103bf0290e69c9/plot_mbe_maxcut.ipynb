{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Basis Encoding\n\nMulti-Basis Encoding ([1]_) (MBE) quantum optimization algorithm for MaxCut using TensorLy-Quantum.\nTensorLy-Quantum provides a Python interface \nto build TT-tensor network circuit simulator \nfor large-scale simulation of variational quantum circuits\nwith full Autograd support similar to traditional PyTorch Neural Networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorly as tl\nimport tlquantum as tlq\nfrom torch import randint, rand, arange, cat, tanh, no_grad, float32\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment the line below to use the GPU\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#device = 'cuda' \ndevice = 'cpu'\n\ndtype = float32\n\nnepochs = 40 #number of training epochs\n\nnqubits = 20 #number of qubits\nncontraq = 2 #2 #number of qubits to pre-contract into single core\nncontral = 2 #2 #number of layers to pre-contract into a single core\nnterms = 20\nlr = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state = tlq.spins_to_tt_state([0 for i in range(nqubits)], device=device, dtype=dtype) # generate generic zero state |00000>\nstate = tlq.qubits_contract(state, ncontraq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we build a random graph with randomly weighted edges.\nNote: MBE allows us to encode two vertices (typically two qubits) into a single qubit using the z and x-axes.\nIf y-axis included, we can encode three vertices per qubit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vertices1 = randint(2*nqubits, (nterms,), device=device) # randomly generated first qubits (vertices) of each two-qubit term (edge)\nvertices2 = randint(2*nqubits, (nterms,), device=device) # randomly generated second qubits (vertices) of each two-qubit term (edge)\nvertices2[vertices2==vertices1] += 1 # because qubits in this graph are randomly generated, eliminate self-interacting terms\nvertices2[vertices2 >= nqubits] = 0\nweights = rand((nterms,), device=device) # randomly generated edge weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RotY1 = tlq.UnaryGatesUnitary(nqubits, ncontraq, device=device, dtype=dtype) #single-qubit rotations about the Y-axis\nRotY2 = tlq.UnaryGatesUnitary(nqubits, ncontraq, device=device, dtype=dtype)\nCZ0 = tlq.BinaryGatesUnitary(nqubits, ncontraq, tlq.cz(device=device, dtype=dtype), 0) # one controlled-z gate for each pair of qubits using even parity (even qubits control)\nunitaries = [RotY1, CZ0, RotY2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "circuit = tlq.TTCircuit(unitaries, ncontraq, ncontral) # build TTCircuit using specified unitaries\nopz, opx = tl.tensor([[1,0],[0,-1]], device=device, dtype=dtype), tl.tensor([[0,1],[1,0]], device=device, dtype=dtype) # measurement operators for MBE\nprint(opz)\nopt = Adam(circuit.parameters(), lr=lr, amsgrad=True) # define PyTorch optimizer\nloss_vec = tl.zeros(nepochs)\ncut_vec = tl.zeros(nepochs)\n\nfor epoch in range(nepochs):\n    # TTCircuit forward pass computes expectation value of single-qubit pauli-z and pauli-x measurements\n    spinsz, spinsx = circuit.forward_single_qubit(state, opz, opx)\n    spins = cat((spinsz, spinsx))\n    nl_spins = tanh(spins) # apply non-linear activation function to measurement results\n    loss = tlq.calculate_cut(nl_spins, vertices1, vertices2, weights) # calculate the loss function using MBE\n    print('Relaxation (raw) loss at epoch ' + str(epoch) + ': ' + str(loss.item()) + '. \\n')\n    with no_grad():\n        cut_vec[epoch] = tlq.calculate_cut(tl.sign(spins), vertices1, vertices2, weights, get_cut=True) #calculate the rounded MaxCut estimate (algorithm's result)\n        print('Rounded MaxCut value (algorithm\\'s solution): ' + str(cut_vec[epoch]) + '. \\n')\n\n    # PyTorch Autograd attends to backwards pass and parameter update\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    loss_vec[epoch] = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.rc('xtick')\nplt.rc('ytick')\nfig, ax1 = plt.subplots()\nax1.plot(loss_vec.detach().numpy(), color='k')\nax2 = ax1.twinx()\nax2.plot(cut_vec.detach().numpy(), color='g')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss', color='k')\nax2.set_ylabel('Cut', color='g')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n.. [1] T. L. Patti, J. Kossaifi, A. Anandkumar, and S. F. Yelin, \"Variational Quantum Optimization with Multi-Basis Encodings,\" (2021), arXiv:2106.13304.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}